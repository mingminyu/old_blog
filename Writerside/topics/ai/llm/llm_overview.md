# 大模型综述

<show-structure depth="3"/>

## 1. 大模型打榜排名

目前中文大模型的打榜排名主要是在 [CEval](https://cevalbenchmark.com/static/leaderboard_zh.html) 以及 [FlagEval](https://flageval.baai.ac.cn/#/trending) 上可以查看，但是对于同一个模型给出的排名结果，实际这两者差异也比较大。

这里以 ChatGLM 模型为例，截至 2023/11 数据，在 FlagEval 上排名第三，但是 CEval 上排名在 11，但是同一模型的不同版本效果提升是可以参考的，比如 CEval 上就可以看到 ChatGLM3 比 ChatGLM2 有着 **50%** 的性能提升。

## 2. 信息抽取能力比较

采用 ZERO-SHOT 策略，测试在同一样本上不同开源 LLM 在信息抽取上的准确性。通过使用 QWen-14B-Chat-Int8、QWen-7B-Chat 以及 ChatGLM3-6B 这 3 个不同大模型，对同一段对话内容进行信息的提取，最终测试效果为: **ChatGLM3-6B >> QWen-14B-Chat-Int8 >> QWen-7B-Chat**，并且 QWen-7B-Chat 的推理速率是远远低于 ChatGLM3-6B 的。

> ChatGLM 团队表示不考虑多轮对话的场景，Base 模型的推理效果是要由于 Chat 模型的，但实测下来并不是，也许是使用上存在一些问题。

## 3. 专业术语

### 3.1 温度(temperature)

在大模型得使用代码中，我们经常会看到通过在 `model.chat` 中调整 `temperature` 参数值来改变生成的文本内容，当其值越接近于 0 时，模型每次生成的文本相对更稳定，当设定为 1 时每次得出的文本差异性会比较大。

> `temperature` 需为一个正数，其值是可以大于 1 的
{style="note"}

`temperature` 是一个控制生成文本多样性的参数，它影响大模型在生成下一个词时对概率分布的敏感程度。具体来说，在给定一个概率分布的情况下，模型在生成下一个词时会从这个分布中进行采样。

通过调整 `temperature` 值，可以平衡所生成文本的多样性和确定性。较大的 `temperature` 值会使分布更加平滑，增加样本的多样性；较小的 `temperature` 值则会使得分布更加尖锐，**更加强调的是概率分布的峰值表现**，生成更加确定性的样本。

举例说明
: 当 `temperature=0` 时，模型的生成行为非常确定，相当于完全消除了随机性，因为它总会选择概率最高的单词，相应地生成的文本也更加集中和确定。
: 当 `temperature=0.5` 时，模型在生成时会保留一些随机性，但也更加倾向于选择**高概率**的单词，生成的文本会在确定性和多样性之间达到一种平衡。 
: 当 `temperature=1` 时，模型的生成将更加随机。它会平等地考虑所有可能的单词，生成的文本将更加多样化和随机。这时模型对于概率分布中的各个单词更加均衡地进行采样。

### 3.2 top_p

`top_p` 是一个用于控制生成文本的参数，它是一种用于截断概率分布的方法，以限制在采样时考虑的概率分布的部分范围。

具体来说，模型在生成下一个词时，会按照概率分布进行采样。而 `top_p` 参数则指定了模型考虑的概率分布的上部分，即在累积概率达到 `top_p` 之后的部分将被**忽略**。这样可以控制生成文本的多样性，防止生成过于碎片化或不连贯的内容。

举例说明
: 当 `top_p=0.8` 时，模型会考虑概率分布中**累积概率最高**的那些单词，直到累积概率达到 **0.8**。超过这个概率的单词将被过滤掉，不纳入采样范围。
: 当 `top_p=1` 时，那么模型将考虑**所有单词**，即不进行截断，采样概率分布的整个范围。


> 实际使用时，我们通常先调整 `temperature`，再通过 `top_p` 细化调整生成的文本。
{style="note"}

### 3.3 top_k

`top_k` 是用于控制生成文本的参数之一，它被用于在采样时限制考虑的词汇范围。具体来说，`top_k` 参数指定了在采样时只考虑概率分布中前 k 个概率最高的单词，而其他单词将被忽略。

举例说明
: 当 `top_k=50` 时，那么模型在生成文本时将只考虑概率分布中概率最高的 50 个单词，而超过这个范围的单词将不会被纳入到采样的选择中。

### 3.4 max_length

`max_length` 用于设定输入文本的处理的最大长度，当超过这个长度时会进行文本截断。

### 3.5 num_beams

`num_beams` 用于**束搜索**（beam search）的参数，控制在生成时考虑的候选序列数。具体来说，`num_beams` 参数指定了在生成时考虑的候选序列的数量，较大的 `num_beams` 值将使得模型在生成时同时考虑更多的候选序列，这可以影响生成文本的多样性和准确性。

> 较大的 `num_beams` 值会增加计算成本，因为需要同时考虑多个候选序列。在资源有限的情况下，需要权衡计算成本和生成文本的质量，请谨慎使用。
{style="warning"}

束搜索
: 束搜索是一种用于生成序列的搜索算法，它在生成文本时考虑多个候选序列，而不仅仅是单一的序列。


### 3.6 do_sample

`do_sample` 用于控制生成文本时是否使用采样的方式。具体而言，当 `do_sample` 设置为 `True` 时，模型在生成文本时会使用采样方法，从概率分布中随机抽样选择下一个词；当设置为 `False` 时，模型将根据概率分布选择最可能的下一个词。

> `do_sample` 的值设定并不会影响 `temperature`、`top_p`、`top_k` 的作用
{style="note"}



<seealso>
<category ref="ref_docs">
    <a href="https://cevalbenchmark.com/static/leaderboard_zh.html">CEval上大模型排名</a>
    <a href="https://flageval.baai.ac.cn/#/trending">FlagEval上大模型排名</a>
    <a href="https://mp.weixin.qq.com/s/wny89l6TtujcBKq_Tms-Mg">终于有人将大模型可视化了</a>
</category>
</seealso>
